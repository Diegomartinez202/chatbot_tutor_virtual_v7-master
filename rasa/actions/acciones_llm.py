# ==========================================================
# actions/actions_llm.py  (VERSI√ìN OPTIMIZADA + ROBUSTA)
# ==========================================================

import os
import re
import logging
import requests
import json
from typing import Any, Text, Dict, List

from rasa_sdk import Action, Tracker
from rasa_sdk.executor import CollectingDispatcher
from rasa_sdk.events import SlotSet

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# ----------------- CONFIG DESDE ENV -----------------
OLLAMA_URL = os.getenv("OLLAMA_URL", "http://ollama:11434")
OLLAMA_MODEL = os.getenv("OLLAMA_MODEL", "llama3")
OLLAMA_MAX_TOKENS = int(os.getenv("OLLAMA_MAX_TOKENS", "350"))
OLLAMA_TIMEOUT = int(os.getenv("OLLAMA_TIMEOUT", "15"))


# ==========================================================
# üî• PROMPT PROFESIONAL PARA UN TUTOR DEL SENA + LLM H√çBRIDO
# ==========================================================
PROMPT_SYSTEM = """
Eres *Tutor Virtual Profesional del SENA*, especializado en formaci√≥n por competencias.
TU MISI√ìN:
- Explicar temas acad√©micos de manera clara, did√°ctica y estructurada.
- Ser amable, profesional y preciso.
- Ajustarte al contexto educativo colombiano.

REGLAS OBLIGATORIAS:
1. Nunca inventes datos institucionales. Si no sabes, responde:
   "No tengo la informaci√≥n exacta; puedo orientarte en el proceso general."
2. No uses lenguaje t√©cnico excesivo; prioriza comprensi√≥n del aprendiz.
3. Si el usuario pregunta sobre certificados, estados acad√©micos o procesos reales:
   ‚Üí NO des datos personales.
   ‚Üí Explica el procedimiento oficial.
4. T√∫ solo puedes responder en uno de estos dos formatos:
   - INTENT:<nombre_intent>
   - RESPUESTA:<texto pedag√≥gico>

ESTRUCTURA DE RESPUESTA (cuando uses RESPUESTA):
1) Definici√≥n breve
2) Pasos claros
3) Ejemplo pr√°ctico
4) Recomendaci√≥n final (siguiente paso sugerido)

Cumple SIEMPRE esta estructura.
"""


# ==========================================================
# üîí ANONIMIZACI√ìN ROBUSTA
# ==========================================================
def anonymize_text(text: str) -> str:
    if not text:
        return text
    text = re.sub(r"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}", "[EMAIL]", text)
    text = re.sub(r"\b\d{6,}\b", "[NUM]", text)
    text = re.sub(r"\b(?:\d[ -]*?){13,19}\b", "[NUM]", text)
    text = re.sub(
        r"\b[A-Z√Å√â√ç√ì√ö][a-z√°√©√≠√≥√∫]+(?:\s[A-Z√Å√â√ç√ì√ö][a-z√°√©√≠√≥√∫]+){0,2}\b",
        "[NAME]", text)
    text = re.sub(
        r"\b(?:calle|cra|carrera|av|avenida|cll)\b[^\n,]{0,40}",
        "[ADDRESS]", text, flags=re.IGNORECASE)
    return text


# ==========================================================
# ‚ö° LLAMADA A OLLAMA (MEJORADA)
# ==========================================================
def call_ollama(prompt: str) -> str:
    url = f"{OLLAMA_URL}/api/generate"
    payload = {
        "model": OLLAMA_MODEL,
        "prompt": prompt,
        "max_tokens": OLLAMA_MAX_TOKENS,
        "temperature": 0.15,
        "top_p": 0.9,
        "repeat_penalty": 1.05,
    }

    try:
        resp = requests.post(url, json=payload, timeout=OLLAMA_TIMEOUT)
        resp.raise_for_status()
        data = resp.json()

        # Diferentes versiones de Ollama ‚Üí manejar todos los formatos
        if isinstance(data, dict):
            for key in ["response", "generated", "result"]:
                if key in data and isinstance(data[key], str):
                    return data[key].strip()

            if "results" in data and isinstance(data["results"], list):
                r0 = data["results"][0]
                for key in ["content", "text", "output"]:
                    if key in r0:
                        return str(r0[key]).strip()

        if isinstance(data, str):
            return data.strip()

        return ""

    except Exception as e:
        logger.exception("‚ùå Error llamando a Ollama")
        return ""


# ==========================================================
# üß† PARSER DE RESPUESTA INTELIGENTE
# ==========================================================
def parse_llm_response(text: str) -> Dict[str, str]:
    if not text:
        return {"type": "raw", "value": ""}

    t = text.strip()

    # Buscar INTENT aunque venga rodeado de texto adicional
    m_int = re.search(r"INTENT\s*:\s*([a-zA-Z0-9_]+)", t, flags=re.I)
    if m_int:
        return {"type": "intent", "value": m_int.group(1).strip()}

    # Buscar RESPUESTA: aunque venga con saltos o espacios
    m_resp = re.search(r"RESPUESTA\s*:\s*(.+)", t, flags=re.I | re.S)
    if m_resp:
        return {"type": "response", "value": m_resp.group(1).strip()}

    # Intentar JSON
    try:
        j = json.loads(t)
        if "intent" in j:
            return {"type": "intent", "value": j["intent"]}
        if "response" in j:
            return {"type": "response", "value": j["response"]}
    except:
        pass

    return {"type": "raw", "value": t}


# ==========================================================
# üéØ ACCI√ìN PRINCIPAL: ActionHandleWithOllama
# ==========================================================
class ActionHandleWithOllama(Action):
    def name(self) -> Text:
        return "action_handle_with_llm"

    # ---- Construcci√≥n del prompt con historial reducido ----
    def build_prompt(self, tracker: Tracker) -> str:
        user_msg = anonymize_text(tracker.latest_message.get("text", ""))
        intent_info = tracker.latest_message.get("intent", {})

        # historial corto (m√°x 6 turnos)
        history = []
        for e in tracker.events[-12:]:
            if e.get("event") == "user":
                history.append("Usuario: " + anonymize_text(e.get("text", "")))
            elif e.get("event") == "bot":
                history.append("Bot: " + str(e.get("text", "")))

        hist_text = "\n".join(history[-6:])

        prompt = (
            PROMPT_SYSTEM
            + "\n\n=== CONTEXTO DE LA CONVERSACI√ìN ===\n"
            + f"√öltimo mensaje del usuario: {user_msg}\n"
            + f"Intent detectado por Rasa: {intent_info.get('name')} "
            + f"(conf={intent_info.get('confidence')})\n"
            + f"Historial breve:\n{hist_text}\n"
            + "\nResponde √öNICAMENTE en formato:\n"
            + "INTENT:<nombre_intent>  o  RESPUESTA:<texto>\n"
        )
        return prompt

    # ---- Ejecuci√≥n principal ----
    def run(
        self,
        dispatcher: CollectingDispatcher,
        tracker: Tracker,
        domain: Dict[Text, Any],
    ) -> List[Dict[Text, Any]]:

        prompt = self.build_prompt(tracker)
        logger.info(f"[LLM PROMPT] {prompt[:400]}...")

        raw = call_ollama(prompt)

        if not raw:
            dispatcher.utter_message(
                text="No puedo procesar tu solicitud en este momento. ¬øPodr√≠as reformularla?"
            )
            return []

        parsed = parse_llm_response(raw)
        logger.info(f"[LLM PARSED] {parsed}")

        # --- Si Ollama sugiere INTENT ---
        if parsed["type"] == "intent":
            dispatcher.utter_message(
                text=f"Entendido, proceder√© con tu solicitud."
            )
            return [
                SlotSet("llm_suggested_intent", parsed["value"]),
                SlotSet("from_llm", True)
            ]

        # --- Si es texto normal ---
        if parsed["type"] == "response":
            dispatcher.utter_message(text=parsed["value"])
            return [SlotSet("from_llm", True)]

        # --- Raw fallback ---
        dispatcher.utter_message(text=parsed["value"])
        return [SlotSet("from_llm", True)]
